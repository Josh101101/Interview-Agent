{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37e65c4",
   "metadata": {},
   "source": [
    "# Modular AI Interview Agent with crewAI\n",
    "This notebook demonstrates a refactor of the Interview Agent system using the crewAI framework for agent/task orchestration.\n",
    "\n",
    "## Steps\n",
    "1. **Setup & Imports**\n",
    "2. **LLM Initialization**\n",
    "3. **Define Agents (Resume Parser, Interviewer, Evaluator)**\n",
    "4. **Define Tasks**\n",
    "5. **Crew Orchestration**\n",
    "6. **Run Example Workflow**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "088a35ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.P.J. Abdul Kalam, the \"Missile Man of India,\" was a visionary scientist and the 11th President of India, known for his contributions to India's space and defense programs and his inspiring leadership. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup & Imports\n",
    "from crewai import Agent, Task, Crew, LLM\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables (for API keys, etc.)\n",
    "load_dotenv()\n",
    "\n",
    "# 2. LLM Initialization\n",
    "llm = LLM(\n",
    "    model=\"groq/gemma2-9b-it\",\n",
    "    temperature=1.0\n",
    ")\n",
    "test_again = llm.call(\"Tell me about APJ Abdul Kalam in one sentence\")\n",
    "print(test_again)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740f12d",
   "metadata": {},
   "source": [
    "## Resume Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50500951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"name\": \"John Doe\",\n",
      "  \"email\": \"john@example.com\",\n",
      "  \"experience\": \"5 years in Python, AI\",\n",
      "  \"skills\": [\"Python\", \"Machine Learning\", \"Data Science\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# 3. Define Resume Parser Agent and Task\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import Optional, Union, List\n",
    "from typing import Optional\n",
    "from crewai import Crew\n",
    "\n",
    "# Candidate profile model (simplified for demo)\n",
    "class CandidateProfile(BaseModel):\n",
    "    name: str = Field(description=\"Candidate's full name\")\n",
    "    email: str = Field(description=\"Candidate's email address\")\n",
    "    experience: Union[str, List] = Field(description=\"experience\")\n",
    "    projects: Union[str, List] = Field(description=\"projects\")\n",
    "    education: Union[str, List] = Field(description=\"education\")\n",
    "    skills: Union[str, List] = Field(description=\"skills\")\n",
    "    extracurricular: Union[str, List] = Field(description=\"extracurricular\")\n",
    "    achievements: Optional[Union[str, List]] = Field(description=\"achievements\")\n",
    "    certifications: Optional[Union[str, List]] = Field(description=\"certifications\")\n",
    "    resume: str = Field(description=\"Resume text or summary\")\n",
    "\n",
    "    @field_validator(\n",
    "        \"experience\", \"projects\", \"education\", \"skills\", \"extracurricular\", \"achievements\", \"certifications\"\n",
    "    )\n",
    "    def join_list_fields(cls, v, info):\n",
    "        if isinstance(v, list):\n",
    "            # If list of dicts, join their string representations\n",
    "            if all(isinstance(i, dict) for i in v):\n",
    "                return \"; \".join([str(i) for i in v])\n",
    "            return \", \".join(str(i) for i in v)\n",
    "        return v\n",
    "\n",
    "# Define the Resume Parser agent\n",
    "resume_parser_agent = Agent(\n",
    "    role=\"Resume Parser\",\n",
    "    goal=\"Extract structured candidate information from resume text\",\n",
    "    backstory=\"You are an expert at reading resumes and extracting key candidate details for interview preparation.\",\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Define the parsing task\n",
    "resume_text = \"\"\"John Doe\n",
    "john@example.com\n",
    "Experience: 5 years in Python, AI\n",
    "Skills: Python, Machine Learning, Data Science\"\"\"\n",
    "\n",
    "parse_resume_task = Task(\n",
    "    description=\"Parse the following resume and extract name, email, experience, and skills as JSON:\\n\" + resume_text,\n",
    "    agent=resume_parser_agent,\n",
    "    expected_output=\"A JSON object with keys: name, email, experience, skills.\"\n",
    ")\n",
    "\n",
    "# Create a crew with the agent and task\n",
    "crew = Crew(\n",
    "    agents=[resume_parser_agent],\n",
    "    tasks=[parse_resume_task]\n",
    ")\n",
    "\n",
    "# Run the workflow\n",
    "results = crew.kickoff()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c85e5e",
   "metadata": {},
   "source": [
    "## Agent 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f908af7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Aditya Lalchandani\",\n",
      "  \"email\": \"aditya.lalchandani17@gmail.com\",\n",
      "  \"experience\": [\n",
      "    {\n",
      "      \"title\": \"British Airways Job Simulation on Forage\",\n",
      "      \"company\": \"British Airways\",\n",
      "      \"dates\": \"January 2025\",\n",
      "      \"description\": \"Virtual Internship\\n•Cleaned and preprocessed over 50,000 airline booking records, engineered new features and removed 1,100+ outliers using\\nZ-score filtering to improve data quality for predictive modeling.\\n•Developed and tuned XGBoost classification models, achieving a recall improvement of 18% for booking completion\\nprediction on test data through advanced feature engineering and hyperparameter optimization.\\n•Visualized and analyzed the importance of 40+ features, identifying purchase lead as the top predictor and providing\\nactionable insights to optimize flight scheduling and marketing strategies.\"\n",
      "    }\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"title\": \"Flight Price Predictor\",\n",
      "      \"description\": \"•Created an end-to-end flight price prediction system using Python and Streamlit, emphasizing modular ML pipelines.\\n•Applied advanced data preprocessing techniques, including feature engineering, encoding, and outlier handling, to\\nprepare diverse datasets for modeling.\\n•Trained and fine-tuned XGBoost regression models, evaluating performance with key metrics, and deployed the full ML\\npipeline for production readiness.\",\n",
      "      \"url\": \"Github\",\n",
      "      \"technologies\": [\n",
      "        \"Python\",\n",
      "        \"Pandas\",\n",
      "        \"Scikit-learn\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Song Recommender System\",\n",
      "      \"description\": \"•Designed and actualized a hybrid music recommender system, integrating collaborative and content-based filtering.\\n•Efficiently processed and manipulated large-scale music datasets using Dask, NumPy, and SciPy to handle big data.\\n•Constructed a responsive Streamlit web application for real-time song recommendations, prioritizing user experience and\\nclean, modular code.\",\n",
      "      \"url\": \"Github\",\n",
      "      \"technologies\": [\n",
      "        \"Python\",\n",
      "        \"Scikit-learn\",\n",
      "        \"Dask\",\n",
      "        \"NumPy\",\n",
      "        \"SciPy\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Quiz Generator App\",\n",
      "      \"description\": \"•Authored an interactive quiz web application using Streamlit, Python, LangChain, and the Groq LLM API to generate\\nAI-powered questions.\\n•Leveraged advanced LLM capabilities to dynamically create unique multiple-choice and fill-in-the-blank questions across\\nvarious subjects and difficulties.\\n•Integrated instant feedback and detailed results for users, enabling them to track progress and export quiz data as CSV.\",\n",
      "      \"url\": \"Github\",\n",
      "      \"technologies\": [\n",
      "        \"Langchain\",\n",
      "        \"Streamlit\",\n",
      "        \"Python\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"education\": [{\n",
      "    \"institution\": \"Thadomal Shahani Engineering College\",\n",
      "    \"degree\": \"Bachelor of Electronics and Telecommunication\",\n",
      "    \"years\": \"2020 – 2024\"\n",
      "  }],\n",
      "  \"skills\": {\n",
      "    \"Languages\": [\n",
      "      \"Python\",\n",
      "      \"HTML/CSS\"\n",
      "    ],\n",
      "    \"Developer Tools\": [\n",
      "      \"Git\",\n",
      "      \"Github\",\n",
      "      \"DVC\",\n",
      "      \"Dagshub\",\n",
      "      \"CI/CD\",\n",
      "      \"Mlflow\"\n",
      "    ],\n",
      "    \"Technologies/Frameworks\": [\n",
      "      \"Pandas\",\n",
      "      \"Numpy\",\n",
      "      \"Scikit-learn\",\n",
      "      \"Pytorch\",\n",
      "      \"Matplotlib\",\n",
      "      \"Seaborn\",\n",
      "      \"LangChain\",\n",
      "      \"Hugging Face\",\n",
      "      \"Tansformers\"\n",
      "    ],\n",
      "    \"Databases\": [\n",
      "      \"SQL\"\n",
      "    ]\n",
      "  },\n",
      "  \"extracurricular\": [{\n",
      "    \"organization\": \"Prashraya Welfare Foundation\",\n",
      "    \"role\": \"Volunteer\",\n",
      "    \"years\": \"June 2023 – July 2023\",\n",
      "    \"description\": \"•Gained practical understanding of nonprofit regulations and honed management and leadership abilities through various\\noperational tasks.\\n•Conducted outreach calls to identify and recruit potential volunteers, effectively communicating the foundation’s mission\\nand needs.\"\n",
      "  }],\n",
      "  \"achievements\": [],\n",
      "  \"certifications\": [\n",
      "    \"Machine Learning Specialization (Coursera)\",\n",
      "    \"Microsoft Certified Azure AI Fundamentals\",\n",
      "    \"Hackerank SQL (Intermediate)\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "\n",
    "def extract_text_from_pdf(file_path: str) -> str:\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(file_path: str) -> str:\n",
    "    doc = Document(file_path)\n",
    "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    return text\n",
    "\n",
    "def get_resume_text(resume_input: str, filetype: str = \"text\") -> str:\n",
    "    if filetype == \"pdf\":\n",
    "        return extract_text_from_pdf(resume_input)\n",
    "    elif filetype == \"docx\":\n",
    "        return extract_text_from_docx(resume_input)\n",
    "    else:\n",
    "        return resume_input  # Assume plain text\n",
    "\n",
    "# Example usage:\n",
    "resume_input = \"D:/Python/AI Agent Project/Interview Agent/notebooks/Aditya Lalchandani Resume.pdf\"  # or .docx or plain text\n",
    "filetype = \"pdf\"  # or \"docx\" or \"text\"\n",
    "resume_text = get_resume_text(resume_input, filetype)\n",
    "\n",
    "parse_resume_task = Task(\n",
    "    description=(\n",
    "        \"Parse the following resume and extract name, email, experience, projects, education, skills, extracurricular, achievements, certifications as JSON:\\n\"\n",
    "        + resume_text\n",
    "    ),\n",
    "    agent=resume_parser_agent,\n",
    "    expected_output=\"A JSON object with keys: name, email, experience, projects, education, skills, extracurricular, achievements, certifications.\"\n",
    ")\n",
    "\n",
    "crew = Crew(\n",
    "    agents=[resume_parser_agent],\n",
    "    tasks=[parse_resume_task]\n",
    ")\n",
    "\n",
    "results = crew.kickoff()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9e991f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd3b09e9",
   "metadata": {},
   "source": [
    "## Agent 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f8d40c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: You mentioned improving recall in your British Airways project. Can you elaborate on the steps you took to achieve this 18% improvement, including the specific feature engineering techniques and hyperparameter tuning strategies used?\n",
      "Q: Your Flight Price Predictor project highlights your experience with modular ML pipelines. Can you describe how you structured your pipeline and the benefits of this approach, particularly in terms of maintainability and scalability?\n",
      "Q: In your Song Recommender System, you mention using both collaborative and content-based filtering. Can you explain the rationale behind this hybrid approach and how you balanced the strengths of each method to create a more robust recommendation system?\n",
      "Q: Your experience with handling large-scale music datasets using Dask is impressive. Can you provide a concrete example of a data processing challenge you encountered while working with these datasets and how you leveraged Dask to overcome it efficiently?\n",
      "Q: You've utilized Streamlit for deploying several projects, including the Flight Price Predictor and Song Recommender System. What factors did you consider when choosing Streamlit as your deployment platform, and how did it contribute to creating user-friendly and interactive applications?\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from crewai import Agent, Task, Crew\n",
    "import json\n",
    "\n",
    "# Pydantic model for a generated interview question\n",
    "class InterviewQuestion(BaseModel):\n",
    "    question: str = Field(description=\"The interview question text\")\n",
    "    answer: str = Field(default=\"\", description=\"The answer to the question\")\n",
    "\n",
    "# Define the Interview Question Generator agent\n",
    "question_agent = Agent(\n",
    "    role=\"Interview Question Generator\",\n",
    "    goal=\"Generate technical, behavioral, situational interview questions based on candidate profile and job description\",\n",
    "    backstory=\"You are an expert interviewer who creates relevant questions for candidates based on the job they are applying for.You have a good understanding of what makes a good candidate. You are very good at asking questions that get to the point.\",\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Inputs\n",
    "job_position = \"Data Scientist\"\n",
    "job_description = (\n",
    "    \"We are seeking a Data Scientist with strong experience in Python, machine learning, and data analysis. \"\n",
    "    \"The candidate should be able to build predictive models, work with large datasets, and communicate insights effectively.\"\n",
    ")\n",
    "num_questions = 5\n",
    "\n",
    "# Use the output from the resume parser (results) as input\n",
    "candidate_profile = results  # or extract the relevant dict if needed\n",
    "\n",
    "question_task = Task(\n",
    "    description=(\n",
    "        f\"You are preparing for an interview for the position of '{job_position}'.\\n\"\n",
    "        f\"Job Description: {job_description}\\n\"\n",
    "        f\"Based on the following candidate profile, generate {num_questions} technical interview questions relevant to their experience, skills, and projects, and the job description above.The goal is to assess the candidate's skills, experience, and suitability for the role \"\n",
    "        \"Return ONLY a JSON list of objects with the key 'question'.\\n\"\n",
    "        f\"Candidate Profile:\\n{candidate_profile}\"\n",
    "    ),\n",
    "    agent=question_agent,\n",
    "    expected_output=\"A JSON list of objects with the key 'question'.\"\n",
    ")\n",
    "\n",
    "crew = Crew(\n",
    "    agents=[question_agent],\n",
    "    tasks=[question_task]\n",
    ")\n",
    "\n",
    "questions_output = crew.kickoff()\n",
    "\n",
    "# --- Robustly extract the JSON string from CrewOutput ---\n",
    "def extract_json_from_output(output):\n",
    "    # Try .result, .output, or str fallback\n",
    "    if hasattr(output, \"result\"):\n",
    "        result = output.result\n",
    "        if isinstance(result, str):\n",
    "            return result\n",
    "        elif isinstance(result, (list, dict)):\n",
    "            return json.dumps(result)\n",
    "    if hasattr(output, \"output\"):\n",
    "        out = output.output\n",
    "        if isinstance(out, str):\n",
    "            return out\n",
    "        elif isinstance(out, (list, dict)):\n",
    "            return json.dumps(out)\n",
    "    # Fallback: try to convert to string\n",
    "    return str(output)\n",
    "\n",
    "# Parse and validate the questions using Pydantic\n",
    "try:\n",
    "    questions_json = extract_json_from_output(questions_output)\n",
    "    # Try to find the first JSON list in the string (handles LLM extra text)\n",
    "    import re\n",
    "    match = re.search(r'\\\\[.*\\\\]', questions_json, re.DOTALL)\n",
    "    if match:\n",
    "        questions_json = match.group(0)\n",
    "    parsed_questions = [InterviewQuestion(**q) for q in json.loads(questions_json)]\n",
    "    for q in parsed_questions:\n",
    "        print(f\"Q: {q.question}\")\n",
    "        q.answer = input(\"Your answer: \")\n",
    "except Exception as e:\n",
    "    print(\"Error parsing questions:\", e)\n",
    "    print(questions_output)\n",
    "\n",
    "# Save questions and answers for next agent\n",
    "answered_questions = [q.model_dump() for q in parsed_questions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ead627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'You mentioned improving recall in your British Airways project. Can you elaborate on the steps you took to achieve this 18% improvement, including the specific feature engineering techniques and hyperparameter tuning strategies used?',\n",
       "  'answer': 'ddd'},\n",
       " {'question': 'Your Flight Price Predictor project highlights your experience with modular ML pipelines. Can you describe how you structured your pipeline and the benefits of this approach, particularly in terms of maintainability and scalability?',\n",
       "  'answer': 'cx'},\n",
       " {'question': 'In your Song Recommender System, you mention using both collaborative and content-based filtering. Can you explain the rationale behind this hybrid approach and how you balanced the strengths of each method to create a more robust recommendation system?',\n",
       "  'answer': 'sx'},\n",
       " {'question': 'Your experience with handling large-scale music datasets using Dask is impressive. Can you provide a concrete example of a data processing challenge you encountered while working with these datasets and how you leveraged Dask to overcome it efficiently?',\n",
       "  'answer': 'f'},\n",
       " {'question': \"You've utilized Streamlit for deploying several projects, including the Flight Price Predictor and Song Recommender System. What factors did you consider when choosing Streamlit as your deployment platform, and how did it contribute to creating user-friendly and interactive applications?\",\n",
       "  'answer': 's'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answered_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dae2459b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: You mentioned improving recall in your British Airways project. Can you elaborate on the steps you took to achieve this 18% improvement, including the specific feature engineering techniques and hyperparameter tuning strategies used?\n",
      "A: ddd\n",
      "Score: 0.0\n",
      "Comments: The answer is incomplete and doesn't provide any relevant information about the British Airways project. To earn a higher score, the candidate should describe the specific feature engineering techniques used (e.g., one-hot encoding, TF-IDF) and how hyperparameter tuning strategies (e.g., grid search, random search) were employed to optimize recall.\n",
      "---\n",
      "Q: Your Flight Price Predictor project highlights your experience with modular ML pipelines. Can you describe how you structured your pipeline and the benefits of this approach, particularly in terms of maintainability and scalability?\n",
      "A: cx\n",
      "Score: 0.0\n",
      "Comments: The answer is incomplete and doesn't provide any details about the structured pipeline or its benefits. A strong answer would outline the components of the pipeline (data ingestion, preprocessing, feature engineering, model training, evaluation, deployment), explain how modularity enhances maintainability (e.g., easy to update individual components), and discuss scalability aspects (e.g., parallelization, infrastructure support).\n",
      "---\n",
      "Q: In your Song Recommender System, you mention using both collaborative and content-based filtering. Can you explain the rationale behind this hybrid approach and how you balanced the strengths of each method to create a more robust recommendation system?\n",
      "A: sx\n",
      "Score: 0.0\n",
      "Comments: The answer is incomplete and lacks details about the hybrid approach. A strong answer would explain the limitations of each filtering method (e.g., cold start problem for collaborative filtering, need for feature engineering for content-based filtering) and how combining them addresses these issues. It should also describe techniques used to balance the contributions of both methods (e.g., weighted averaging, rule-based systems).\n",
      "---\n",
      "Q: Your experience with handling large-scale music datasets using Dask is impressive. Can you provide a concrete example of a data processing challenge you encountered while working with these datasets and how you leveraged Dask to overcome it efficiently?\n",
      "A: f\n",
      "Score: 0.0\n",
      "Comments: The answer is incomplete and doesn't provide specific details about the data processing challenge or how Dask was used to overcome it. A strong answer would describe a tangible challenge (e.g., memory limitations, slow computation times) and explain how Dask's parallel processing capabilities (e.g., distributed dataframes, task scheduling) addressed this challenge efficiently. It could mention specific Dask functionalities used (e.g., `dask.parallel.map`, `dask.dataframe`) and the performance improvement achieved.\n",
      "---\n",
      "Q: You've utilized Streamlit for deploying several projects, including the Flight Price Predictor and Song Recommender System. What factors did you consider when choosing Streamlit as your deployment platform, and how did it contribute to creating user-friendly and interactive applications?\n",
      "A: s\n",
      "Score: 0.0\n",
      "Comments: The answer is incomplete and lacks specific details about the factors considered and Streamlit's contributions. A strong answer would highlight Streamlit's ease of use for prototyping and deploying ML applications, its intuitive interface for creating interactive widgets, and its ability to generate responsive web applications. It could also mention other factors like community support, documentation, and deployment options.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from crewai import Agent, Task, Crew\n",
    "import json\n",
    "\n",
    "# Pydantic model for feedback\n",
    "class AnswerFeedback(BaseModel):\n",
    "    question: str = Field(description=\"The interview question text\")\n",
    "    answer: str = Field(description=\"The candidate's answer\")\n",
    "    score: float = Field(description=\"Score for the answer (0-10)\")\n",
    "    comments: str = Field(description=\"Feedback comments\")\n",
    "\n",
    "# Define the Answer Evaluation agent\n",
    "evaluation_agent = Agent(\n",
    "    role=\"Answer Evaluation Agent\",\n",
    "    goal=\"Evaluate candidate answers and provide feedback using LLM\",\n",
    "    backstory=\"You are an expert technical interviewer who scores and comments on candidate answers.\",\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Prepare evaluation task input\n",
    "feedback_task = Task(\n",
    "    description=(\n",
    "        \"You are an expert technical interviewer. For each question and answer pair below, provide a score (0-10) and a short feedback comment. \"\n",
    "        \"Return ONLY a JSON list of objects with keys: question, answer, score, comments.\\n\"\n",
    "        f\"Q&A Pairs: {json.dumps(answered_questions, ensure_ascii=False)}\"\n",
    "    ),\n",
    "    agent=evaluation_agent,\n",
    "    expected_output=\"A JSON list of objects with keys: question, answer, score, comments.\"\n",
    ")\n",
    "\n",
    "crew = Crew(\n",
    "    agents=[evaluation_agent],\n",
    "    tasks=[feedback_task]\n",
    ")\n",
    "\n",
    "feedback_output = crew.kickoff()\n",
    "\n",
    "def extract_json_from_output(output):\n",
    "    if hasattr(output, \"result\"):\n",
    "        result = output.result\n",
    "        if isinstance(result, str):\n",
    "            return result\n",
    "        elif isinstance(result, (list, dict)):\n",
    "            return json.dumps(result)\n",
    "    if hasattr(output, \"output\"):\n",
    "        out = output.output\n",
    "        if isinstance(out, str):\n",
    "            return out\n",
    "        elif isinstance(out, (list, dict)):\n",
    "            return json.dumps(out)\n",
    "    return str(output)\n",
    "\n",
    "# Parse and validate feedback\n",
    "try:\n",
    "    feedback_json = extract_json_from_output(feedback_output)\n",
    "    import re\n",
    "    match = re.search(r'\\[.*\\]', feedback_json, re.DOTALL)\n",
    "    if match:\n",
    "        feedback_json = match.group(0)\n",
    "    parsed_feedback = [AnswerFeedback(**f) for f in json.loads(feedback_json)]\n",
    "    for f in parsed_feedback:\n",
    "        print(f\"Q: {f.question}\\nA: {f.answer}\\nScore: {f.score}\\nComments: {f.comments}\\n---\")\n",
    "except Exception as e:\n",
    "    print(\"Error parsing feedback:\", e)\n",
    "    print(feedback_output)\n",
    "\n",
    "# Save feedback for further use\n",
    "feedback_results = [f.model_dump() for f in parsed_feedback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add7a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a066d168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f690cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "\n",
    "def extract_text_from_pdf(file_path: str) -> str:\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(file_path: str) -> str:\n",
    "    doc = Document(file_path)\n",
    "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    return text\n",
    "\n",
    "def get_resume_text(resume_input: str, filetype: str = \"text\") -> str:\n",
    "    if filetype == \"pdf\":\n",
    "        return extract_text_from_pdf(resume_input)\n",
    "    elif filetype == \"docx\":\n",
    "        return extract_text_from_docx(resume_input)\n",
    "    else:\n",
    "        return resume_input  # Assume plain text\n",
    "\n",
    "# Example usage:\n",
    "resume_input = \"D:/Python/AI Agent Project/Interview Agent/Roshni Sarda Resume.pdf\"\n",
    "filetype = \"pdf\"  # or \"docx\" or \"text\"\n",
    "resume_text = get_resume_text(resume_input, filetype)\n",
    "parse_resume_task = Task(\n",
    "    description=\"Parse the following resume and extract name, email, experience, projects, education,skills, extracurricular, achievements, certifications as JSON:\\n\" + resume_text,\n",
    "    agent=resume_parser_agent,\n",
    "    expected_output=\"A JSON object with keys: name, email, experience, skills.\"\n",
    ")\n",
    "\n",
    "crew = Crew(\n",
    "    agents=[resume_parser_agent],\n",
    "    tasks=[parse_resume_task]\n",
    ")\n",
    "\n",
    "results = crew.kickoff()\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
